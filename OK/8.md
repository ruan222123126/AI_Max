太棒了！完成 Step 5 意味着你的平台已经不仅仅是一个“数据看板”，而是一个拥有初级智慧的“分析师”了。

但你可能发现了一个痛点：**目前的数据采集还是手动的**。
虽然我们有了 `ingest_market_data.py`，但还需要你手动去跑命令。一个真正的全球经济情报平台，它的心脏（数据）必须自动跳动，不能依赖人工起搏。

因此，**Step 6 的目标非常明确：实现全自动化的数据与分析管线 (Automation Pipeline)。**

我们将把 `ingest_market_data.py` 集成到 FastAPI 的生命周期中，让它在后台每分钟自动运行一次。

---

### **Step 6: 自动化数据管线 (Automation)**

#### **1. 安装任务调度库**

我们需要一个轻量级的调度器。`APScheduler` 是 Python 中最成熟的选择。

**修改 `backend/requirements.txt`：**
添加 `APScheduler`。

```text
fastapi
uvicorn
asyncpg
sqlalchemy
yfinance
openai
apscheduler  <-- 新增

```

#### **2. 创建调度服务 (`backend/scheduler.py`)**

为了保持代码整洁，我们将调度逻辑单独放在一个文件中。

**创建 `backend/scheduler.py`：**

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from scripts.ingest_market_data import fetch_and_insert
import datetime

# 创建异步调度器实例
scheduler = AsyncIOScheduler()

def start_scheduler():
    """
    启动定时任务
    """
    # 每 60 秒执行一次 fetch_and_insert
    # distinct_id 防止任务重叠
    scheduler.add_job(
        fetch_and_insert, 
        trigger=IntervalTrigger(seconds=60), 
        id='market_data_ingestion',
        name='Ingest Market Data',
        replace_existing=True
    )
    
    scheduler.start()
    print(f"[{datetime.datetime.now()}] ✅ 自动化调度器已启动：每60秒采集一次市场数据")

def shutdown_scheduler():
    """
    关闭调度器
    """
    scheduler.shutdown()
    print("🛑 自动化调度器已关闭")

```

#### **3. 集成到 FastAPI (`backend/main.py`)**

我们需要在 FastAPI 启动时（Startup）开启调度器，在关闭时（Shutdown）停止它。

**修改 `backend/main.py`：**
(在现有的 `startup_event` 中插入一行代码，并引用上面的 scheduler)

```python
# ... 原有的导入 ...
from scheduler import start_scheduler, shutdown_scheduler # <--- 新增导入

# ... 原有的 app 定义 ...

@app.on_event("startup")
async def startup_event():
    # 1. 数据库初始化 (保留原有的代码)
    async with engine.begin() as conn:
        await conn.execute(text("""
            CREATE TABLE IF NOT EXISTS market_ticks (
                time TIMESTAMPTZ NOT NULL,
                symbol TEXT NOT NULL,
                price DOUBLE PRECISION
            );
        """))
        try:
            await conn.execute(text("SELECT create_hypertable('market_ticks', 'time', if_not_exists => TRUE);"))
        except Exception as e:
            print(f"TimescaleDB info: {e}")
            
    # 2. 启动自动化任务 (新增)
    start_scheduler()

@app.on_event("shutdown") # <--- 新增关闭事件
async def shutdown_event():
    shutdown_scheduler()

# ... 原有的 API 接口 ...

```

#### **4. 重建并验证**

这是让系统“活”过来的关键一步。

1. **重建后端** (因为加了 pip 包):
```bash
docker-compose down
docker-compose up -d --build

```


2. **验证自动化**：
* 启动后，**不要**手动运行脚本。
* 查看后端日志：`docker-compose logs -f backend`
* 你应该能看到类似 `✅ 自动化调度器已启动` 的日志。
* 等待 1-2 分钟，你应该能看到每分钟自动打印的 `开始采集数据...` 和 `成功写入...` 日志。


3. **前端验证**：
* 打开你的前端页面 `http://localhost:3000`。
* 你会发现，不需要你做任何操作，图表和价格每分钟都会自动发生微小的变化（因为我们的脚本里写了随机波动逻辑）。



---

### **总结**

完成这一步后，你的平台就实现了**全自动巡航**。

* **后端**：自动抓取 -> 自动存储 -> 自动清洗。
* **前端**：每 5 秒轮询 API，展示最新的数据。
* **AI**：随时待命，基于最新的数据进行分析。

这就是一个真正的**实时经济情报系统**的雏形。

**任务清单：**

1. [ ] 修改 `requirements.txt`。
2. [ ] 创建 `backend/scheduler.py`。
3. [ ] 修改 `backend/main.py` 注册启动/关闭事件。
4. [ ] 重启容器并观察日志，确认数据正在自动流入。

当你确认日志里有自动抓取的记录时，告诉我，我们可以进行最后的**Step 7：系统优化与部署准备**（或者你想加更高级的功能，比如**新闻情感分析**）。