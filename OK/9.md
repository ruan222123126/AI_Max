太棒了！看到你已经完成了 Step 7（这一步通常是系统优化或自动化脚本 `start.sh` 的配置，或者是项目结构的整理），这标志着你的平台已经从一个“原型”变成了一个可运行、易维护的“系统”。

现在的状态：

* **数据**：有价格（Price）。
* **分析**：AI 能基于价格评判涨跌。
* **缺憾**：**AI 是“瞎”的**。它知道“比特币跌了 5%”，但不知道“为什么跌”。是因为美联储加息？还是因为交易所被黑？

**Step 8 的目标：构建“全球资讯雷达” (News Intelligence)。**
我们将接入实时财经新闻源，并利用 AI 进行**情感分析 (Sentiment Analysis)**，让你的平台不仅能看盘，还能看新闻。

---

### **Step 8: 财经新闻采集与情感分析 (News Ingestion & Analysis)**

我们将使用 **RSS 订阅**（最稳定且免费的方式）来抓取全球主流财经媒体（如 Yahoo Finance, CNBC）的快讯。

#### **1. 添加爬虫依赖**

我们需要 `feedparser` 来解析 RSS，`textblob` 或简单的 NLP 逻辑来做预处理（或者直接丢给 LLM）。

**修改 `backend/requirements.txt`：**
添加以下两行：

```text
feedparser
beautifulsoup4

```

#### **2. 更新数据库结构 (`backend/main.py`)**

我们需要一张新表来存新闻。

**修改 `backend/main.py` 的 `startup_event`：**
在创建 `market_ticks` 的代码下方，增加 `financial_news` 表的创建逻辑：

```python
    # ... 原有的 market_ticks 创建代码 ...

    # 2. 创建新闻表
    async with engine.begin() as conn:
        await conn.execute(text("""
            CREATE TABLE IF NOT EXISTS financial_news (
                id SERIAL PRIMARY KEY,
                published_at TIMESTAMPTZ NOT NULL,
                title TEXT NOT NULL,
                source TEXT,
                url TEXT UNIQUE,
                sentiment_score FLOAT  -- 预留给 AI 打分 (-1.0 到 1.0)
            );
        """))

```

#### **3. 编写新闻采集脚本 (`backend/scripts/ingest_news.py`)**

这是一个全新的脚本。

**创建 `backend/scripts/ingest_news.py`：**

```python
import asyncio
import os
import feedparser
from datetime import datetime
from email.utils import parsedate_to_datetime
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy import text

# RSS 源列表 (Yahoo Finance)
RSS_URLS = [
    "https://finance.yahoo.com/news/rssindex",
    # 你可以添加更多，比如 Investing.com 的 RSS
]

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+asyncpg://user:password@db:5432/economy_data")

async def fetch_news():
    print(f"[{datetime.now()}] 正在扫描全球财经新闻...")
    
    engine = create_async_engine(DATABASE_URL, echo=False)
    news_items = []

    for url in RSS_URLS:
        try:
            feed = feedparser.parse(url)
            print(f"  -> 从 {url} 获取到 {len(feed.entries)} 条新闻")
            
            for entry in feed.entries[:5]: # 每次只取最新的 5 条防止重复太多
                # 解析时间
                if hasattr(entry, 'published'):
                    pub_date = parsedate_to_datetime(entry.published)
                else:
                    pub_date = datetime.now()
                
                news_items.append({
                    "published_at": pub_date,
                    "title": entry.title,
                    "source": "Yahoo Finance",
                    "url": entry.link,
                    "sentiment_score": 0.0 # 默认为中性，后续由 AI 分析
                })
        except Exception as e:
            print(f"  ❌ RSS 解析失败: {e}")

    # 写入数据库 (使用 ON CONFLICT DO NOTHING 忽略重复 URL)
    async with engine.begin() as conn:
        for item in news_items:
            try:
                await conn.execute(text("""
                    INSERT INTO financial_news (published_at, title, source, url, sentiment_score)
                    VALUES (:published_at, :title, :source, :url, :sentiment_score)
                    ON CONFLICT (url) DO NOTHING;
                """), item)
            except Exception as e:
                print(f"写入错误: {e}")
    
    print(f"✅ 新闻入库完成")
    await engine.dispose()

if __name__ == "__main__":
    asyncio.run(fetch_news())

```

#### **4. 挂载到调度器 (`backend/scheduler.py`)**

让新闻也自动更新。建议频率低一点，比如每 5 分钟一次。

**修改 `backend/scheduler.py`：**

```python
from scripts.ingest_news import fetch_news # <--- 导入新脚本

# ... 在 start_scheduler 函数中添加 ...

    # 新闻采集任务：每 5 分钟 (300秒) 执行一次
    scheduler.add_job(
        fetch_news, 
        trigger=IntervalTrigger(seconds=300), 
        id='news_ingestion',
        name='Ingest Global News',
        replace_existing=True
    )

```

#### **5. 升级 AI 大脑 (`backend/ai_service.py`)**

这是最酷的一步。我们要让 AI 在分析时，不仅看价格，还要看最近的新闻标题。

**修改 `generate_market_report` 函数：**
(你需要修改 SQL 查询逻辑，增加一段查新闻的代码)

```python
# ... 这是一个代码片段思路，你需要整合进你的函数 ...

# 1. 查询最近 5 条新闻
news_query = text("SELECT title FROM financial_news ORDER BY published_at DESC LIMIT 5;")
news_result = await conn.execute(news_query)
news_titles = [row.title for row in news_result]
news_context = "\n".join([f"- {t}" for t in news_titles])

# 2. 注入到 Prompt
prompt = f"""
...
【最新市场新闻】：
{news_context}

请结合上述价格走势和新闻标题，分析市场情绪...
"""

```

---

**执行清单：**

1. [ ] 修改 `requirements.txt` 并重建容器 (`docker-compose build backend`)。
2. [ ] 修改 `main.py` 增加新闻表。
3. [ ] 创建 `ingest_news.py` 脚本。
4. [ ] 注册到 `scheduler.py`。
5. [ ] (选做) 修改 `ai_service.py` 让 AI 读取新闻。

完成后，你的 AI 就会说：“比特币今日下跌，**可能受到‘SEC 推迟 ETF 决议’这一新闻的影响**...”。这才是真正的智能。去试试吧！